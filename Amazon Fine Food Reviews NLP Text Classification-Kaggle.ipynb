{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction and Motivation\n",
    "This problem dataset was taken from Kaggle website under the name \"Amazon Fine Food Reviews\". The motivation was to have an hands-on experience with a good Natural Language Processing problem along with sentiment analysis. Using the analysis in this work, a prediction model can be made to predict a recommendation/sentiment as either positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Context\n",
    "The dataset used in this exercise was taken from Kaggle website. The dataset consists of ~500,000 reviews that were submitted over a span of more than 10 years. These reviews includes product and user information, ratings, and text review. Moreover, reviews from all other Amazon categories are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing useful libraries and data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve,auc,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "5                     0                       0      4  1342051200   \n",
       "6                     0                       0      5  1340150400   \n",
       "7                     0                       0      5  1336003200   \n",
       "8                     1                       1      5  1322006400   \n",
       "9                     0                       0      5  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp=pd.read_csv('Reviews.csv')\n",
    "df_temp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains many columns such as \"Id\", \"ProductId\", \"UserId\" \"ProfileName\" and \"Time\" which might be useful for some exploratory analysis along based on specific users and their preferences for some particular product type. For the purpose of text classification task where a particular review will be classified as either a \"Positive\" class or a \"Negative\" class, these columns are of not much use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for our text classification task of a review, we will try to use supervised learning where we will use \"Summary\" columns as our features and \"Score\" columns as class labels. We could have used the \"Text\" column as well along with \"Summary\" column  which contains a more detailed review for a particluar product for a better analysis but I will go on with only \"Summary\" as of now. Moreover, for simplicity purpose, we will convert our Scores as \"positive\" for scores higher than or equal to 3 and \"negative\" to all cases otherwise. This is make this classification problem from mulitclass classification to binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice Taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Yay Barley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                        Summary\n",
       "0      5                          Good Quality Dog Food\n",
       "1      1                              Not as Advertised\n",
       "2      4                          \"Delight\" says it all\n",
       "3      2                                 Cough Medicine\n",
       "4      5                                    Great taffy\n",
       "5      4                                     Nice Taffy\n",
       "6      5  Great!  Just as good as the expensive brands!\n",
       "7      5                         Wonderful, tasty taffy\n",
       "8      5                                     Yay Barley\n",
       "9      5                               Healthy Dog Food"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_temp.iloc[:,[6,8]]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to takes scores ranging from 1 to 5 and returns them as positive or negative score as discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of training a supervised learning algorithm on our data, we first need to separate our data in features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Score=df['Score']\n",
    "Score=Score.map(review)\n",
    "Summary=df['Summary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's split our data in training and test datasets with 80% for the purpose of training and 20% for the purpose of validating our models. For this, we will use Scikit-learn train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Score=df['Score']\n",
    "Score=Score.map(review)\n",
    "Summary=df['Summary']\n",
    "X_train,X_test,y_train,y_test=train_test_split(Summary,Score,\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Score                                            Summary\n",
      "0   positive                              Good Quality Dog Food\n",
      "1   negative                                  Not as Advertised\n",
      "2   positive                              \"Delight\" says it all\n",
      "3   negative                                     Cough Medicine\n",
      "4   positive                                        Great taffy\n",
      "5   positive                                         Nice Taffy\n",
      "6   positive      Great!  Just as good as the expensive brands!\n",
      "7   positive                             Wonderful, tasty taffy\n",
      "8   positive                                         Yay Barley\n",
      "9   positive                                   Healthy Dog Food\n",
      "10  positive                    The Best Hot Sauce in the World\n",
      "11  positive  My cats LOVE this \"diet\" food better than thei...\n",
      "12  negative               My Cats Are Not Fans of the New Food\n",
      "13  positive                                  fresh and greasy!\n",
      "14  positive                       Strawberry Twizzlers - Yummy\n",
      "15  positive           Lots of twizzlers, just what you expect.\n",
      "16  negative                                         poor taste\n",
      "17  positive                                           Love it!\n",
      "18  positive                                 GREAT SWEET CANDY!\n",
      "19  positive                            Home delivered twizlers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Score']=df['Score'].map(review)\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now splitted our datasets in training and test sets and transformed our integer score as an opinion: positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing the data\n",
    "\n",
    "We can see in our \"Summary\" column that users have submitted their reviews in various styles using upper-case and punctuations. We need to process them to a common (lower-case) case. The purpose for doing this is to make the models interprets \"great\" and \"GREAT\" as a same entity. We will use python re module to get rid of punctuations and upper-case letters from our summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['Summary'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Not as Advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Cough Medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>Great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>Nice Taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Yay Barley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                        Summary\n",
       "0  positive                          Good Quality Dog Food\n",
       "1  negative                              Not as Advertised\n",
       "2  positive                          \"Delight\" says it all\n",
       "3  negative                                 Cough Medicine\n",
       "4  positive                                    Great taffy\n",
       "5  positive                                     Nice Taffy\n",
       "6  positive  Great!  Just as good as the expensive brands!\n",
       "7  positive                         Wonderful, tasty taffy\n",
       "8  positive                                     Yay Barley\n",
       "9  positive                               Healthy Dog Food"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text=re.sub('<[^>]*>','',str(text))\n",
    "    text = re.sub(r'\\<a href', ' ',str(text))\n",
    "    text = re.sub(r'&amp;', '', str(text))\n",
    "    text=re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ',str(text))\n",
    "    text = re.sub(r'\\'', ' ', str(text))\n",
    "    text=re.sub('[\\W]+',' ',text.lower())\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Summary']=df['Summary'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>delight says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>nice taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>great just as good as the expensive brands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful tasty taffy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>yay barley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>healthy dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>positive</td>\n",
       "      <td>the best hot sauce in the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>positive</td>\n",
       "      <td>my cats love this diet food better than their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>negative</td>\n",
       "      <td>my cats are not fans of the new food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>positive</td>\n",
       "      <td>fresh and greasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>positive</td>\n",
       "      <td>strawberry twizzlers yummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>positive</td>\n",
       "      <td>lots of twizzlers just what you expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>negative</td>\n",
       "      <td>poor taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>positive</td>\n",
       "      <td>love it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>positive</td>\n",
       "      <td>great sweet candy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>positive</td>\n",
       "      <td>home delivered twizlers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score                                            Summary\n",
       "0   positive                              good quality dog food\n",
       "1   negative                                  not as advertised\n",
       "2   positive                                delight says it all\n",
       "3   negative                                     cough medicine\n",
       "4   positive                                        great taffy\n",
       "5   positive                                         nice taffy\n",
       "6   positive        great just as good as the expensive brands \n",
       "7   positive                              wonderful tasty taffy\n",
       "8   positive                                         yay barley\n",
       "9   positive                                   healthy dog food\n",
       "10  positive                    the best hot sauce in the world\n",
       "11  positive  my cats love this diet food better than their ...\n",
       "12  negative               my cats are not fans of the new food\n",
       "13  positive                                  fresh and greasy \n",
       "14  positive                         strawberry twizzlers yummy\n",
       "15  positive            lots of twizzlers just what you expect \n",
       "16  negative                                         poor taste\n",
       "17  positive                                           love it \n",
       "18  positive                                 great sweet candy \n",
       "19  positive                            home delivered twizlers"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will utilize NLTK library to tokenize words, remove stopwords and perform stemming of words using PorterStemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "\n",
    "def tokenize_porter(text):\n",
    "    return [porter.stem(word) for word in text.split() ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a separate (dummy) Summary column to see the results of our text preprocessing steps and to make sure our tokenizer and tokenize_porter functions are working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Summary1']=df['Summary'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>[good, quality, dog, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>[not, as, advertised]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>[delight, says, it, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>[cough, medicine]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>[great, taffy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>nice taffy</td>\n",
       "      <td>[nice, taffy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>great just as good as the expensive brands</td>\n",
       "      <td>[great, just, as, good, as, the, expensive, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful tasty taffy</td>\n",
       "      <td>[wonderful, tasty, taffy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>yay barley</td>\n",
       "      <td>[yay, barley]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>healthy dog food</td>\n",
       "      <td>[healthy, dog, food]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                      Summary  \\\n",
       "0  positive                        good quality dog food   \n",
       "1  negative                            not as advertised   \n",
       "2  positive                          delight says it all   \n",
       "3  negative                               cough medicine   \n",
       "4  positive                                  great taffy   \n",
       "5  positive                                   nice taffy   \n",
       "6  positive  great just as good as the expensive brands    \n",
       "7  positive                        wonderful tasty taffy   \n",
       "8  positive                                   yay barley   \n",
       "9  positive                             healthy dog food   \n",
       "\n",
       "                                            Summary1  \n",
       "0                         [good, quality, dog, food]  \n",
       "1                              [not, as, advertised]  \n",
       "2                           [delight, says, it, all]  \n",
       "3                                  [cough, medicine]  \n",
       "4                                     [great, taffy]  \n",
       "5                                      [nice, taffy]  \n",
       "6  [great, just, as, good, as, the, expensive, br...  \n",
       "7                          [wonderful, tasty, taffy]  \n",
       "8                                      [yay, barley]  \n",
       "9                               [healthy, dog, food]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['Summary1']=df['Summary'].apply(tokenize_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>[good, qualiti, dog, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>[not, as, advertis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>[delight, say, it, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>[cough, medicin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>[great, taffi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>nice taffy</td>\n",
       "      <td>[nice, taffi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>great just as good as the expensive brands</td>\n",
       "      <td>[great, just, as, good, as, the, expens, brand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful tasty taffy</td>\n",
       "      <td>[wonder, tasti, taffi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>yay barley</td>\n",
       "      <td>[yay, barley]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>healthy dog food</td>\n",
       "      <td>[healthi, dog, food]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                      Summary  \\\n",
       "0  positive                        good quality dog food   \n",
       "1  negative                            not as advertised   \n",
       "2  positive                          delight says it all   \n",
       "3  negative                               cough medicine   \n",
       "4  positive                                  great taffy   \n",
       "5  positive                                   nice taffy   \n",
       "6  positive  great just as good as the expensive brands    \n",
       "7  positive                        wonderful tasty taffy   \n",
       "8  positive                                   yay barley   \n",
       "9  positive                             healthy dog food   \n",
       "\n",
       "                                          Summary1  \n",
       "0                       [good, qualiti, dog, food]  \n",
       "1                              [not, as, advertis]  \n",
       "2                          [delight, say, it, all]  \n",
       "3                                 [cough, medicin]  \n",
       "4                                   [great, taffi]  \n",
       "5                                    [nice, taffi]  \n",
       "6  [great, just, as, good, as, the, expens, brand]  \n",
       "7                           [wonder, tasti, taffi]  \n",
       "8                                    [yay, barley]  \n",
       "9                             [healthi, dog, food]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be working as expected. So let's proceed with our biulding our text classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Mining and Classification task can be computationally  very expensive and generally with datasets like this, the computation time might limit us from trying various models for supervised learning. Hence, to address this issue, we will first try building sample models on a smaller subset and the model with best performance will then be scaled to entire dataset.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 560,000 entries for our entire dataset. Let's begin by selecting 50,000 rows selected at random in sample dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142237</td>\n",
       "      <td>positive</td>\n",
       "      <td>misleading description deceptive pictures</td>\n",
       "      <td>[mislead, descript, decept, pictur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97664</td>\n",
       "      <td>positive</td>\n",
       "      <td>love it</td>\n",
       "      <td>[love, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536349</td>\n",
       "      <td>positive</td>\n",
       "      <td>good deal</td>\n",
       "      <td>[good, deal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519875</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious and nutritious</td>\n",
       "      <td>[delici, and, nutriti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>472474</td>\n",
       "      <td>positive</td>\n",
       "      <td>excellent coffee in a great portable package</td>\n",
       "      <td>[excel, coffe, in, a, great, portabl, packag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>538298</td>\n",
       "      <td>positive</td>\n",
       "      <td>pleasant earl grey</td>\n",
       "      <td>[pleasant, earl, grey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>446156</td>\n",
       "      <td>positive</td>\n",
       "      <td>love having the ability to get a great cup of ...</td>\n",
       "      <td>[love, have, the, abil, to, get, a, great, cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>387717</td>\n",
       "      <td>positive</td>\n",
       "      <td>we love these</td>\n",
       "      <td>[we, love, these]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>147156</td>\n",
       "      <td>positive</td>\n",
       "      <td>soy sauce alternative with a healthy kick</td>\n",
       "      <td>[soy, sauc, altern, with, a, healthi, kick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>557741</td>\n",
       "      <td>negative</td>\n",
       "      <td>overpriced</td>\n",
       "      <td>[overpr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>483434</td>\n",
       "      <td>positive</td>\n",
       "      <td>absolutely great</td>\n",
       "      <td>[absolut, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18218</td>\n",
       "      <td>positive</td>\n",
       "      <td>dehydrated peanut butter</td>\n",
       "      <td>[dehydr, peanut, butter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>429395</td>\n",
       "      <td>positive</td>\n",
       "      <td>european formula is delicious</td>\n",
       "      <td>[european, formula, is, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>470092</td>\n",
       "      <td>positive</td>\n",
       "      <td>canine treats</td>\n",
       "      <td>[canin, treat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>57712</td>\n",
       "      <td>positive</td>\n",
       "      <td>these truly are the best gummi bears in the wo...</td>\n",
       "      <td>[these, truli, are, the, best, gummi, bear, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100565</td>\n",
       "      <td>positive</td>\n",
       "      <td>very good and healthy treat</td>\n",
       "      <td>[veri, good, and, healthi, treat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>413819</td>\n",
       "      <td>positive</td>\n",
       "      <td>i hope they are as good for you as they taste</td>\n",
       "      <td>[i, hope, they, are, as, good, for, you, as, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>454829</td>\n",
       "      <td>negative</td>\n",
       "      <td>awful there s better out there</td>\n",
       "      <td>[aw, there, s, better, out, there]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>416195</td>\n",
       "      <td>positive</td>\n",
       "      <td>tasty product</td>\n",
       "      <td>[tasti, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>298074</td>\n",
       "      <td>positive</td>\n",
       "      <td>smart fries best snack ever</td>\n",
       "      <td>[smart, fri, best, snack, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>132111</td>\n",
       "      <td>positive</td>\n",
       "      <td>good tea</td>\n",
       "      <td>[good, tea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>308281</td>\n",
       "      <td>positive</td>\n",
       "      <td>great natural snack</td>\n",
       "      <td>[great, natur, snack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>105855</td>\n",
       "      <td>positive</td>\n",
       "      <td>these are delicious</td>\n",
       "      <td>[these, are, delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>78781</td>\n",
       "      <td>positive</td>\n",
       "      <td>ella s kitchen organic the red one</td>\n",
       "      <td>[ella, s, kitchen, organ, the, red, one]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>474557</td>\n",
       "      <td>negative</td>\n",
       "      <td>godiva coffee pods</td>\n",
       "      <td>[godiva, coffe, pod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>396450</td>\n",
       "      <td>positive</td>\n",
       "      <td>very healthy satisfying</td>\n",
       "      <td>[veri, healthi, satisfi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>479724</td>\n",
       "      <td>positive</td>\n",
       "      <td>easier to use than standard vacu vin</td>\n",
       "      <td>[easier, to, use, than, standard, vacu, vin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>193834</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious</td>\n",
       "      <td>[delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>146473</td>\n",
       "      <td>negative</td>\n",
       "      <td>onslow says yuck</td>\n",
       "      <td>[onslow, say, yuck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16422</td>\n",
       "      <td>negative</td>\n",
       "      <td>do not buy from mike s grocers</td>\n",
       "      <td>[do, not, buy, from, mike, s, grocer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>394668</td>\n",
       "      <td>positive</td>\n",
       "      <td>great product</td>\n",
       "      <td>[great, product]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>460904</td>\n",
       "      <td>negative</td>\n",
       "      <td>this is english tea from virginia</td>\n",
       "      <td>[thi, is, english, tea, from, virginia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49972</th>\n",
       "      <td>294719</td>\n",
       "      <td>positive</td>\n",
       "      <td>tangy smoked woody taste</td>\n",
       "      <td>[tangi, smoke, woodi, tast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49973</th>\n",
       "      <td>478867</td>\n",
       "      <td>positive</td>\n",
       "      <td>greasy ok but not the stellar chili i expected</td>\n",
       "      <td>[greasi, ok, but, not, the, stellar, chili, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49974</th>\n",
       "      <td>97547</td>\n",
       "      <td>positive</td>\n",
       "      <td>misleading</td>\n",
       "      <td>[mislead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49975</th>\n",
       "      <td>547610</td>\n",
       "      <td>positive</td>\n",
       "      <td>from a picky eater i love these</td>\n",
       "      <td>[from, a, picki, eater, i, love, these]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49976</th>\n",
       "      <td>470355</td>\n",
       "      <td>positive</td>\n",
       "      <td>just like my greatgrandmother used to make</td>\n",
       "      <td>[just, like, my, greatgrandmoth, use, to, make]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49977</th>\n",
       "      <td>47903</td>\n",
       "      <td>positive</td>\n",
       "      <td>coconut water yoo hoo</td>\n",
       "      <td>[coconut, water, yoo, hoo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>240498</td>\n",
       "      <td>positive</td>\n",
       "      <td>real pasta</td>\n",
       "      <td>[real, pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>234631</td>\n",
       "      <td>positive</td>\n",
       "      <td>so much fun</td>\n",
       "      <td>[so, much, fun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>349468</td>\n",
       "      <td>positive</td>\n",
       "      <td>great flavor and organic</td>\n",
       "      <td>[great, flavor, and, organ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>265830</td>\n",
       "      <td>positive</td>\n",
       "      <td>nonni s are great</td>\n",
       "      <td>[nonni, s, are, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>228166</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious</td>\n",
       "      <td>[delici]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49983</th>\n",
       "      <td>567505</td>\n",
       "      <td>negative</td>\n",
       "      <td>not much cherry almond flavor</td>\n",
       "      <td>[not, much, cherri, almond, flavor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49984</th>\n",
       "      <td>504590</td>\n",
       "      <td>positive</td>\n",
       "      <td>sweet sugar free</td>\n",
       "      <td>[sweet, sugar, free]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49985</th>\n",
       "      <td>391545</td>\n",
       "      <td>negative</td>\n",
       "      <td>terrible</td>\n",
       "      <td>[terribl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49986</th>\n",
       "      <td>389470</td>\n",
       "      <td>positive</td>\n",
       "      <td>awesome snack</td>\n",
       "      <td>[awesom, snack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49987</th>\n",
       "      <td>431653</td>\n",
       "      <td>positive</td>\n",
       "      <td>great food</td>\n",
       "      <td>[great, food]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49988</th>\n",
       "      <td>437263</td>\n",
       "      <td>positive</td>\n",
       "      <td>herbal alternative to caffeinated beverages a ...</td>\n",
       "      <td>[herbal, altern, to, caffein, beverag, a, diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49989</th>\n",
       "      <td>21518</td>\n",
       "      <td>positive</td>\n",
       "      <td>the pet dog is delighted too</td>\n",
       "      <td>[the, pet, dog, is, delight, too]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>438078</td>\n",
       "      <td>positive</td>\n",
       "      <td>champagne of honeys</td>\n",
       "      <td>[champagn, of, honey]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>491932</td>\n",
       "      <td>negative</td>\n",
       "      <td>not so fresh as promised</td>\n",
       "      <td>[not, so, fresh, as, promis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>525882</td>\n",
       "      <td>negative</td>\n",
       "      <td>trans fats</td>\n",
       "      <td>[tran, fat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>431122</td>\n",
       "      <td>negative</td>\n",
       "      <td>you have to be kidding me</td>\n",
       "      <td>[you, have, to, be, kid, me]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>409248</td>\n",
       "      <td>positive</td>\n",
       "      <td>delicious and so easy to prepare</td>\n",
       "      <td>[delici, and, so, easi, to, prepar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>490353</td>\n",
       "      <td>positive</td>\n",
       "      <td>yum</td>\n",
       "      <td>[yum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>337187</td>\n",
       "      <td>positive</td>\n",
       "      <td>stevia is great</td>\n",
       "      <td>[stevia, is, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>360362</td>\n",
       "      <td>positive</td>\n",
       "      <td>healthy tasty and multi functional</td>\n",
       "      <td>[healthi, tasti, and, multi, function]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>414894</td>\n",
       "      <td>negative</td>\n",
       "      <td>weak uninspiring coffee is the result i got</td>\n",
       "      <td>[weak, uninspir, coffe, is, the, result, i, got]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>154505</td>\n",
       "      <td>positive</td>\n",
       "      <td>good coffee</td>\n",
       "      <td>[good, coffe]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index     Score                                            Summary  \\\n",
       "0      142237  positive          misleading description deceptive pictures   \n",
       "1       97664  positive                                            love it   \n",
       "2      536349  positive                                         good deal    \n",
       "3      519875  positive                          delicious and nutritious    \n",
       "4      472474  positive       excellent coffee in a great portable package   \n",
       "5      538298  positive                                 pleasant earl grey   \n",
       "6      446156  positive  love having the ability to get a great cup of ...   \n",
       "7      387717  positive                                      we love these   \n",
       "8      147156  positive          soy sauce alternative with a healthy kick   \n",
       "9      557741  negative                                        overpriced    \n",
       "10     483434  positive                                  absolutely great    \n",
       "11      18218  positive                           dehydrated peanut butter   \n",
       "12     429395  positive                      european formula is delicious   \n",
       "13     470092  positive                                      canine treats   \n",
       "14      57712  positive  these truly are the best gummi bears in the wo...   \n",
       "15     100565  positive                        very good and healthy treat   \n",
       "16     413819  positive     i hope they are as good for you as they taste    \n",
       "17     454829  negative                    awful there s better out there    \n",
       "18     416195  positive                                      tasty product   \n",
       "19     298074  positive                       smart fries best snack ever    \n",
       "20     132111  positive                                           good tea   \n",
       "21     308281  positive                                great natural snack   \n",
       "22     105855  positive                                these are delicious   \n",
       "23      78781  positive                 ella s kitchen organic the red one   \n",
       "24     474557  negative                                 godiva coffee pods   \n",
       "25     396450  positive                            very healthy satisfying   \n",
       "26     479724  positive               easier to use than standard vacu vin   \n",
       "27     193834  positive                                         delicious    \n",
       "28     146473  negative                                   onslow says yuck   \n",
       "29      16422  negative                    do not buy from mike s grocers    \n",
       "...       ...       ...                                                ...   \n",
       "49970  394668  positive                                      great product   \n",
       "49971  460904  negative                 this is english tea from virginia    \n",
       "49972  294719  positive                          tangy smoked woody taste    \n",
       "49973  478867  positive     greasy ok but not the stellar chili i expected   \n",
       "49974   97547  positive                                         misleading   \n",
       "49975  547610  positive                    from a picky eater i love these   \n",
       "49976  470355  positive         just like my greatgrandmother used to make   \n",
       "49977   47903  positive                             coconut water yoo hoo    \n",
       "49978  240498  positive                                         real pasta   \n",
       "49979  234631  positive                                       so much fun    \n",
       "49980  349468  positive                          great flavor and organic    \n",
       "49981  265830  positive                                  nonni s are great   \n",
       "49982  228166  positive                                          delicious   \n",
       "49983  567505  negative                      not much cherry almond flavor   \n",
       "49984  504590  positive                                   sweet sugar free   \n",
       "49985  391545  negative                                          terrible    \n",
       "49986  389470  positive                                      awesome snack   \n",
       "49987  431653  positive                                        great food    \n",
       "49988  437263  positive  herbal alternative to caffeinated beverages a ...   \n",
       "49989   21518  positive                      the pet dog is delighted too    \n",
       "49990  438078  positive                                champagne of honeys   \n",
       "49991  491932  negative                           not so fresh as promised   \n",
       "49992  525882  negative                                         trans fats   \n",
       "49993  431122  negative                         you have to be kidding me    \n",
       "49994  409248  positive                   delicious and so easy to prepare   \n",
       "49995  490353  positive                                                yum   \n",
       "49996  337187  positive                                    stevia is great   \n",
       "49997  360362  positive                 healthy tasty and multi functional   \n",
       "49998  414894  negative        weak uninspiring coffee is the result i got   \n",
       "49999  154505  positive                                       good coffee    \n",
       "\n",
       "                                                Summary1  \n",
       "0                    [mislead, descript, decept, pictur]  \n",
       "1                                             [love, it]  \n",
       "2                                           [good, deal]  \n",
       "3                                 [delici, and, nutriti]  \n",
       "4          [excel, coffe, in, a, great, portabl, packag]  \n",
       "5                                 [pleasant, earl, grey]  \n",
       "6      [love, have, the, abil, to, get, a, great, cup...  \n",
       "7                                      [we, love, these]  \n",
       "8            [soy, sauc, altern, with, a, healthi, kick]  \n",
       "9                                               [overpr]  \n",
       "10                                      [absolut, great]  \n",
       "11                              [dehydr, peanut, butter]  \n",
       "12                       [european, formula, is, delici]  \n",
       "13                                        [canin, treat]  \n",
       "14     [these, truli, are, the, best, gummi, bear, in...  \n",
       "15                     [veri, good, and, healthi, treat]  \n",
       "16     [i, hope, they, are, as, good, for, you, as, t...  \n",
       "17                    [aw, there, s, better, out, there]  \n",
       "18                                      [tasti, product]  \n",
       "19                       [smart, fri, best, snack, ever]  \n",
       "20                                           [good, tea]  \n",
       "21                                 [great, natur, snack]  \n",
       "22                                  [these, are, delici]  \n",
       "23              [ella, s, kitchen, organ, the, red, one]  \n",
       "24                                  [godiva, coffe, pod]  \n",
       "25                              [veri, healthi, satisfi]  \n",
       "26          [easier, to, use, than, standard, vacu, vin]  \n",
       "27                                              [delici]  \n",
       "28                                   [onslow, say, yuck]  \n",
       "29                 [do, not, buy, from, mike, s, grocer]  \n",
       "...                                                  ...  \n",
       "49970                                   [great, product]  \n",
       "49971            [thi, is, english, tea, from, virginia]  \n",
       "49972                        [tangi, smoke, woodi, tast]  \n",
       "49973  [greasi, ok, but, not, the, stellar, chili, i,...  \n",
       "49974                                          [mislead]  \n",
       "49975            [from, a, picki, eater, i, love, these]  \n",
       "49976    [just, like, my, greatgrandmoth, use, to, make]  \n",
       "49977                         [coconut, water, yoo, hoo]  \n",
       "49978                                      [real, pasta]  \n",
       "49979                                    [so, much, fun]  \n",
       "49980                        [great, flavor, and, organ]  \n",
       "49981                             [nonni, s, are, great]  \n",
       "49982                                           [delici]  \n",
       "49983                [not, much, cherri, almond, flavor]  \n",
       "49984                               [sweet, sugar, free]  \n",
       "49985                                          [terribl]  \n",
       "49986                                    [awesom, snack]  \n",
       "49987                                      [great, food]  \n",
       "49988  [herbal, altern, to, caffein, beverag, a, diff...  \n",
       "49989                  [the, pet, dog, is, delight, too]  \n",
       "49990                              [champagn, of, honey]  \n",
       "49991                       [not, so, fresh, as, promis]  \n",
       "49992                                        [tran, fat]  \n",
       "49993                       [you, have, to, be, kid, me]  \n",
       "49994                [delici, and, so, easi, to, prepar]  \n",
       "49995                                              [yum]  \n",
       "49996                                [stevia, is, great]  \n",
       "49997             [healthi, tasti, and, multi, function]  \n",
       "49998   [weak, uninspir, coffe, is, the, result, i, got]  \n",
       "49999                                      [good, coffe]  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledf=df.sample(n=50000)\n",
    "sampledf.reset_index(inplace=True)\n",
    "sampledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to split our data in training and test datasets with 80% for the purpose of training and 20% for the purpose of validating our models. For this, we will use Scikit-learn train_test_split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Score=sampledf['Score']\n",
    "Summary=sampledf['Summary']\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(Summary,Score,\n",
    "                                               test_size=0.2,\n",
    "                                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(10000,)\n",
      "(40000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Not Used\n",
    "X_train=sampledf.loc[:150000,'Summary'].values\n",
    "y_train=sampledf.loc[:150000,'Score'].values\n",
    "X_test=sampledf.loc[150000:,'Summary'].values\n",
    "y_test=sampledf.loc[150000:,'Score'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150001,)\n",
      "(50000,)\n",
      "(150001,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "#Not Used\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,)\n",
      "(10000,)\n",
      "(40000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's start by building our first model using SVM. We will deploy grid search for finding some starting parameters related to TfIdfVectorizer and SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None]},\n",
    "            {'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 94.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Set: {'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]}\n",
      "Time Taken :  6489.705521821976 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "svm_tfidf=Pipeline([('vect',tfidf),('clf',SVC(random_state=0))])\n",
    "\n",
    "gs_svm_tfidf=GridSearchCV(svm_tfidf,param_grid,scoring='accuracy',\n",
    "                        cv=3,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_svm_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "print('Best Parameter Set: %s'%gs_svm_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_1-SVM Accu. Score : 0.843 \n",
      "Model_1 F1 Score : 0.843 \n",
      "Model_1 Precision Score : 0.843 \n",
      "Model_1 Recall Score : 0.843 \n",
      "Model_1 Confusion Matrix\n",
      " [[    0  7841]\n",
      " [    0 42159]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_svm=gs_svm_tfidf.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_svm,normalize=True)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_svm,average='micro')))\n",
    "print('Model_1 Precision Score : %.3f ' %precision_score(y_test,prediction_svm,average='micro'))\n",
    "print('Model_1 Recall Score : %.3f ' %recall_score(y_test,prediction_svm,average='micro'))\n",
    "print('Model_1 Confusion Matrix\\n' ,confusion_matrix(y_test,prediction_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-1 with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 13.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "\n",
      "Best Parameter Set: {'clf__C': 1.0, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer at 0x000002132B0131E0>}\n",
      "Time Taken :  847.9667613506317 sec\n",
      "Model_1-SVM Accu. Score : 0.858 \n",
      "Model_1 F1 Score : 0.858 \n",
      "Model_1 Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.00      0.00      1423\n",
      "   positive       0.86      1.00      0.92      8577\n",
      "\n",
      "avg / total       0.74      0.86      0.79     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "            'clf__C':[1.0,10.0]}]\n",
    "\n",
    "\n",
    "#Model-2 with SVM and tokenizer and tokenizer_porter in grid_search\n",
    "\n",
    "svm_tfidf_1=Pipeline([('vect',tfidf),('clf',SVC(random_state=0))])\n",
    "\n",
    "gs_svm_tfidf_1=GridSearchCV(svm_tfidf_1,param_grid,scoring='accuracy',\n",
    "                            cv=4,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_svm_tfidf_1.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "\n",
    "print('Training Complete\\n')\n",
    "print('Best Parameter Set: %s'%gs_svm_tfidf_1.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "prediction_svm_1=gs_svm_tfidf_1.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_svm_1)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_svm_1,average='micro')))\n",
    "print('Model_1 Confusion Matrix\\n' ,classification_report(y_test,prediction_svm_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR SVM MODEL-1 training complete........"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model based on linear SVM gave us a classification accuracy score of about 85.8%. Using best parameter set, we see that linear SVM is also using stopwords for analysis.\n",
    "Let's retrain our model with no stopwords this time and see if we can gain any performance over the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 2 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "\n",
      "Best Parameter Set: {'clf__C': 1.0, 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000002132B0131E0>}\n",
      "Time Taken :  249.6503291130066 sec\n",
      "Model_1-SVM Accu. Score : 0.858 \n",
      "Model_1 F1 Score : 0.858 \n",
      "Model_1 Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.00      0.00      0.00      1423\n",
      "   positive       0.86      1.00      0.92      8577\n",
      "\n",
      "avg / total       0.74      0.86      0.79     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yarathor\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "            'clf__C':[1.0]}]\n",
    "\n",
    "\n",
    "#Model-2 with SVM and tokenizer and tokenizer_porter in grid_search\n",
    "\n",
    "svm_tfidf_1=Pipeline([('vect',tfidf),('clf',SVC(random_state=0))])\n",
    "\n",
    "gs_svm_tfidf_1=GridSearchCV(svm_tfidf_1,param_grid,scoring='accuracy',\n",
    "                            cv=4,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_svm_tfidf_1.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "\n",
    "print('Training Complete\\n')\n",
    "print('Best Parameter Set: %s'%gs_svm_tfidf_1.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "prediction_svm_1=gs_svm_tfidf_1.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_svm_1)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_svm_1,average='micro')))\n",
    "print('Model_1 Confusion Matrix\\n' ,classification_report(y_test,prediction_svm_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most research papers have suggested using Bernoulli's Naive Bayes Classifier for text classification, let's give it a try to assess its performance with respect to linear SVM classifier. The reason for high popularity of NB models is that they are simple probablistic models which requires counts of words and n-grams in hash tables. Each word count is treated as separate feature and hence, the feature space consist of total number of distinct words in our entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-2 with BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 8 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Set: {'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenize_porter at 0x000002132B2616A8>}\n",
      "Time Taken :  64.49752116203308 sec\n",
      "Model_1-SVM Accu. Score : 0.885 \n",
      "Model_1 F1 Score : 0.885 \n",
      "Model_1 Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.69      0.35      0.46      1423\n",
      "   positive       0.90      0.97      0.94      8577\n",
      "\n",
      "avg / total       0.87      0.89      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "             'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenize_porter]},\n",
    "            {'vect__ngram_range':[(1,1)],\n",
    "             'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None]}]\n",
    "\n",
    "\n",
    "\n",
    "ber_nb_tfidf=Pipeline([('vect',tfidf),('clf',BernoulliNB())])\n",
    "\n",
    "gs_nb_tfidf=GridSearchCV(ber_nb_tfidf,param_grid,scoring='accuracy',\n",
    "                         cv=4,verbose=1,n_jobs=1)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "gs_nb_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "print('Best Parameter Set: %s'%gs_nb_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "prediction_bnb=gs_nb_tfidf.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_bnb,normalize=True)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_bnb,average='micro')))\n",
    "print('Model_1 Confusion Matrix\\n' ,classification_report(y_test,prediction_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight increment in accuracy of approx. 3 percent when using Bernoulli Naive-Bayes. However, it should be noted that BernoulliNB is best suited for binary/boolean features as mentioned on scikit-learn documentation website. A more suitable classifier of text classification purpose is Multinomial Naive-Bayes which is suitable for classification with discrete features. Hence, keeping this is mind, let's give MultinomialNB a try and see if there are any performance gains over BernoulliNB and linearSVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-3 with MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "\n",
      "Best Parameter Set: {'vect__ngram_range': (1, 1), 'vect__norm': None, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000002132B0131E0>, 'vect__use_idf': False}\n",
      "Time Taken :  46.048134088516235 sec\n",
      "Model_1-SVM Accu. Score : 0.896 \n",
      "Model_1 F1 Score : 0.896 \n",
      "Model_1 Confusion Matrix\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.71      0.45      0.55      1423\n",
      "   positive       0.91      0.97      0.94      8577\n",
      "\n",
      "avg / total       0.89      0.90      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter]},\n",
    "            {'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],             \n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None]}]\n",
    "\n",
    "\n",
    "\n",
    "multi_nb_tfidf=Pipeline([('vect',tfidf),('clf',MultinomialNB())])\n",
    "\n",
    "gs_mnb_tfidf=GridSearchCV(multi_nb_tfidf,param_grid,scoring='accuracy',\n",
    "                        cv=3,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_mnb_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "\n",
    "print('Training Complete\\n')\n",
    "print('Best Parameter Set: %s'%gs_mnb_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "\n",
    "\n",
    "prediction_Mnb_1=gs_mnb_tfidf.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_Mnb_1,normalize=True)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_Mnb_1,average='micro')))\n",
    "print('Model_1 Confusion Matrix\\n' ,classification_report(y_test,prediction_Mnb_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, MultinomialNB did worked slightly better than BernoulliNB but it should be noted that we are training our models using less than 10% of our total available data. Using all the available data is always advisable and can give far better results than above. We will try to build one last model using Logistic Regression since logistic regression is equally suggested for text classification tasks as naive-bayes and can perform even better than naive-bayes in some text-classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-4 using Log-Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000002AB302D4510>}\n",
      "718.2834570407867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                      lowercase=False,\n",
    "                      preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "            'clf__penalty':['l1','l2'],\n",
    "            'clf__C':[1.0,10.0]},\n",
    "           {'vect__ngram_range': [(1,1)],\n",
    "            'vect__stop_words': [stop, None],\n",
    "            'vect__tokenizer': [tokenizer,tokenize_porter],\n",
    "            'vect__use_idf':[False],\n",
    "            'vect__norm':[None],\n",
    "            'clf__penalty': ['l1', 'l2'],\n",
    "            'clf__C': [1.0, 10.0]}]\n",
    "                \n",
    "lr_tfidf=Pipeline([('vect',tfidf),\n",
    "                   ('clf',LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf=GridSearchCV(lr_tfidf,param_grid,\n",
    "                         scoring='accuracy',\n",
    "                         cv=4,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "\n",
    "print('Training Complete\\n')\n",
    "print('Best Parameter Set: %s'%gs_lr_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Woah--90% accuracy...lets push it more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter]},\n",
    "            {'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],             \n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None]}]\n",
    "\n",
    "\n",
    "\n",
    "multi_nb_tfidf=Pipeline([('vect',tfidf),('clf',MultinomialNB())])\n",
    "\n",
    "gs_mnb_tfidf=GridSearchCV(multi_nb_tfidf,param_grid,scoring='accuracy',\n",
    "                        cv=3,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_mnb_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "print('Best Parameter Set: %s'%gs_mnb_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "\n",
    "\n",
    "prediction_Mnb_1=gs_mnb_tfidf.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_Mnb_1,normalize=True)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_Mnb_1,average='micro')))\n",
    "print('Model_1 Precision Score : %.3f ' %precision_score(y_test,prediction_Mnb_1,average='micro'))\n",
    "print('Model_1 Recall Score : %.3f ' %recall_score(y_test,prediction_Mnb_1,average='micro'))\n",
    "print('Model_1 Confusion Matrix\\n' ,confusion_matrix(y_test,prediction_Mnb_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Train Accu. Score : 0.946 \n",
      "LogReg Train F1 Score : 0.946 \n",
      "LogReg Train Precision Score : 0.946 \n",
      "LogReg Train Recall Score : 0.946 \n",
      "LogReg Train Confusion Matrix\n",
      " [[ 17777   5588]\n",
      " [  2529 124107]]\n",
      "LogReg Test Accu. Score : 0.926 \n",
      "LogReg Test F1 Score : 0.926 \n",
      "LogReg Test Precision Score : 0.926 \n",
      "LogReg Test Recall Score : 0.926 \n",
      "LogReg Test Confusion Matrix\n",
      " [[ 5261  2503]\n",
      " [ 1220 41016]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "prediction_lr=gs_lr_tfidf.predict(X_train)\n",
    "\n",
    "print('LogReg Train Accu. Score : %.3f ' %(accuracy_score(y_train,prediction_lr,normalize=True)))\n",
    "print('LogReg Train F1 Score : %.3f ' %(f1_score(y_train,prediction_lr,average='micro')))\n",
    "print('LogReg Train Precision Score : %.3f ' %precision_score(y_train,prediction_lr,average='micro'))\n",
    "print('LogReg Train Recall Score : %.3f ' %recall_score(y_train,prediction_lr,average='micro'))\n",
    "print('LogReg Train Confusion Matrix\\n' ,confusion_matrix(y_train,prediction_lr))\n",
    "\n",
    "\n",
    "prediction_lr=gs_lr_tfidf.predict(X_test)\n",
    "\n",
    "print('LogReg Test Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_lr,normalize=True)))\n",
    "print('LogReg Test F1 Score : %.3f ' %(f1_score(y_test,prediction_lr,average='micro')))\n",
    "print('LogReg Test Precision Score : %.3f ' %precision_score(y_test,prediction_lr,average='micro'))\n",
    "print('LogReg Test Recall Score : %.3f ' %recall_score(y_test,prediction_lr,average='micro'))\n",
    "print('LogReg Test Confusion Matrix\\n' ,confusion_matrix(y_test,prediction_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   45.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Set: {'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenize_porter at 0x000002132B2616A8>}\n",
      "Time Taken :  48.87475395202637 sec\n",
      "Model_1-SVM Accu. Score : 0.885 \n",
      "Model_1 F1 Score : 0.885 \n",
      "Model_1 Precision Score : 0.885 \n",
      "Model_1 Recall Score : 0.885 \n",
      "Model_1 Confusion Matrix\n",
      " [[ 496  927]\n",
      " [ 223 8354]]\n"
     ]
    }
   ],
   "source": [
    "#Model-1 with BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,\n",
    "                     lowercase=False,\n",
    "                     preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],\n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter]},\n",
    "            {'vect__ngram_range':[(1,1)],\n",
    "            'vect__stop_words':[stop,None],             \n",
    "            'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None]}]\n",
    "\n",
    "\n",
    "\n",
    "ber_nb_tfidf=Pipeline([('vect',tfidf),('clf',BernoulliNB())])\n",
    "\n",
    "gs_bnb_tfidf=GridSearchCV(ber_nb_tfidf,param_grid,scoring='accuracy',\n",
    "                        cv=3,verbose=1,n_jobs=1)\n",
    "\n",
    "start=time.time()\n",
    "gs_bnb_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "print('Best Parameter Set: %s'%gs_bnb_tfidf.best_params_)\n",
    "print('Time Taken : ',end-start,'sec')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "prediction_bnb_1=gs_bnb_tfidf.predict(X_test)\n",
    "\n",
    "print('Model_1-SVM Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_bnb_1,normalize=True)))\n",
    "print('Model_1 F1 Score : %.3f ' %(f1_score(y_test,prediction_bnb_1,average='micro')))\n",
    "print('Model_1 Precision Score : %.3f ' %precision_score(y_test,prediction_bnb_1,average='micro'))\n",
    "print('Model_1 Recall Score : %.3f ' %recall_score(y_test,prediction_bnb_1,average='micro'))\n",
    "print('Model_1 Confusion Matrix\\n' ,confusion_matrix(y_test,prediction_bnb_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Summary1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258745</td>\n",
       "      <td>positive</td>\n",
       "      <td>definitely a new cereal</td>\n",
       "      <td>[definit, a, new, cereal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20568</td>\n",
       "      <td>negative</td>\n",
       "      <td>hated it</td>\n",
       "      <td>[hate, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514420</td>\n",
       "      <td>positive</td>\n",
       "      <td>best treats ever</td>\n",
       "      <td>[best, treat, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29245</td>\n",
       "      <td>positive</td>\n",
       "      <td>perfect pumpkin</td>\n",
       "      <td>[perfect, pumpkin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224240</td>\n",
       "      <td>positive</td>\n",
       "      <td>anytime of day</td>\n",
       "      <td>[anytim, of, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>143690</td>\n",
       "      <td>positive</td>\n",
       "      <td>i am sutisufied with my purchase</td>\n",
       "      <td>[i, am, sutisufi, with, my, purchas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>184981</td>\n",
       "      <td>positive</td>\n",
       "      <td>rj s soft black licorice</td>\n",
       "      <td>[rj, s, soft, black, licoric]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60248</td>\n",
       "      <td>positive</td>\n",
       "      <td>exquisitely tempting</td>\n",
       "      <td>[exquisit, tempt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>131954</td>\n",
       "      <td>positive</td>\n",
       "      <td>cheap and tasty</td>\n",
       "      <td>[cheap, and, tasti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>310032</td>\n",
       "      <td>positive</td>\n",
       "      <td>good lord thats a lot of bay leafs</td>\n",
       "      <td>[good, lord, that, a, lot, of, bay, leaf]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>421023</td>\n",
       "      <td>positive</td>\n",
       "      <td>not the kind of smoked salmon i wanted</td>\n",
       "      <td>[not, the, kind, of, smoke, salmon, i, want]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>246737</td>\n",
       "      <td>positive</td>\n",
       "      <td>good stuff</td>\n",
       "      <td>[good, stuff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>467560</td>\n",
       "      <td>negative</td>\n",
       "      <td>diamond pet foods recalled</td>\n",
       "      <td>[diamond, pet, food, recal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>512825</td>\n",
       "      <td>positive</td>\n",
       "      <td>love lavazza</td>\n",
       "      <td>[love, lavazza]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>312829</td>\n",
       "      <td>positive</td>\n",
       "      <td>provolone vs provolone</td>\n",
       "      <td>[provolon, vs, provolon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>498891</td>\n",
       "      <td>positive</td>\n",
       "      <td>ex s cat likes it need i say more</td>\n",
       "      <td>[ex, s, cat, like, it, need, i, say, more]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17355</td>\n",
       "      <td>positive</td>\n",
       "      <td>good tea reusable canisters</td>\n",
       "      <td>[good, tea, reusabl, canist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90371</td>\n",
       "      <td>positive</td>\n",
       "      <td>great deal i love these bars</td>\n",
       "      <td>[great, deal, i, love, these, bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6158</td>\n",
       "      <td>negative</td>\n",
       "      <td>tinged with vitamin c</td>\n",
       "      <td>[ting, with, vitamin, c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>261032</td>\n",
       "      <td>positive</td>\n",
       "      <td>yum eee</td>\n",
       "      <td>[yum, eee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>216517</td>\n",
       "      <td>positive</td>\n",
       "      <td>tug jug dog toy</td>\n",
       "      <td>[tug, jug, dog, toy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>381787</td>\n",
       "      <td>positive</td>\n",
       "      <td>trident val u pack</td>\n",
       "      <td>[trident, val, u, pack]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>283486</td>\n",
       "      <td>positive</td>\n",
       "      <td>my 10th favorite flavor</td>\n",
       "      <td>[my, 10th, favorit, flavor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>236757</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>[wonder]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>425748</td>\n",
       "      <td>positive</td>\n",
       "      <td>crackers o boy</td>\n",
       "      <td>[cracker, o, boy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>314525</td>\n",
       "      <td>positive</td>\n",
       "      <td>fantastic option for the vegetarian baby and c...</td>\n",
       "      <td>[fantast, option, for, the, vegetarian, babi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>53457</td>\n",
       "      <td>positive</td>\n",
       "      <td>excellent healthy food for your dog</td>\n",
       "      <td>[excel, healthi, food, for, your, dog]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>462286</td>\n",
       "      <td>positive</td>\n",
       "      <td>yummi for the cats</td>\n",
       "      <td>[yummi, for, the, cat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>94868</td>\n",
       "      <td>positive</td>\n",
       "      <td>yum</td>\n",
       "      <td>[yum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>525514</td>\n",
       "      <td>positive</td>\n",
       "      <td>interesting packaging</td>\n",
       "      <td>[interest, packag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399970</th>\n",
       "      <td>496507</td>\n",
       "      <td>negative</td>\n",
       "      <td>warning very high sodium content</td>\n",
       "      <td>[warn, veri, high, sodium, content]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399971</th>\n",
       "      <td>350747</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like the taste</td>\n",
       "      <td>[i, like, the, tast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399972</th>\n",
       "      <td>467156</td>\n",
       "      <td>positive</td>\n",
       "      <td>everytime fabulous risotto</td>\n",
       "      <td>[everytim, fabul, risotto]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399973</th>\n",
       "      <td>26817</td>\n",
       "      <td>positive</td>\n",
       "      <td>just like the theater</td>\n",
       "      <td>[just, like, the, theater]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399974</th>\n",
       "      <td>99050</td>\n",
       "      <td>positive</td>\n",
       "      <td>love it</td>\n",
       "      <td>[love, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399975</th>\n",
       "      <td>347328</td>\n",
       "      <td>positive</td>\n",
       "      <td>great catnip seeds</td>\n",
       "      <td>[great, catnip, seed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399976</th>\n",
       "      <td>105166</td>\n",
       "      <td>positive</td>\n",
       "      <td>a wonderful little cookie</td>\n",
       "      <td>[a, wonder, littl, cooki]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399977</th>\n",
       "      <td>397313</td>\n",
       "      <td>positive</td>\n",
       "      <td>good on almost everything</td>\n",
       "      <td>[good, on, almost, everyth]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399978</th>\n",
       "      <td>196485</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "      <td>[great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399979</th>\n",
       "      <td>444966</td>\n",
       "      <td>positive</td>\n",
       "      <td>way to make converts</td>\n",
       "      <td>[way, to, make, convert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399980</th>\n",
       "      <td>377692</td>\n",
       "      <td>positive</td>\n",
       "      <td>great hot chocolate</td>\n",
       "      <td>[great, hot, chocol]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399981</th>\n",
       "      <td>482186</td>\n",
       "      <td>positive</td>\n",
       "      <td>perfect for the hot tea drinker</td>\n",
       "      <td>[perfect, for, the, hot, tea, drinker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399982</th>\n",
       "      <td>484316</td>\n",
       "      <td>positive</td>\n",
       "      <td>tastes great</td>\n",
       "      <td>[tast, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399983</th>\n",
       "      <td>419313</td>\n",
       "      <td>negative</td>\n",
       "      <td>i love dark coffee but this isn t the one for me</td>\n",
       "      <td>[i, love, dark, coffe, but, thi, isn, t, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399984</th>\n",
       "      <td>466307</td>\n",
       "      <td>positive</td>\n",
       "      <td>my favorite drink</td>\n",
       "      <td>[my, favorit, drink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399985</th>\n",
       "      <td>48371</td>\n",
       "      <td>negative</td>\n",
       "      <td>close call</td>\n",
       "      <td>[close, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399986</th>\n",
       "      <td>365437</td>\n",
       "      <td>negative</td>\n",
       "      <td>should have listened to the warnings</td>\n",
       "      <td>[should, have, listen, to, the, warn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399987</th>\n",
       "      <td>458304</td>\n",
       "      <td>positive</td>\n",
       "      <td>the best steak seasoning</td>\n",
       "      <td>[the, best, steak, season]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399988</th>\n",
       "      <td>442488</td>\n",
       "      <td>positive</td>\n",
       "      <td>dee lish</td>\n",
       "      <td>[dee, lish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399989</th>\n",
       "      <td>51984</td>\n",
       "      <td>positive</td>\n",
       "      <td>better treat than meal</td>\n",
       "      <td>[better, treat, than, meal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399990</th>\n",
       "      <td>267349</td>\n",
       "      <td>positive</td>\n",
       "      <td>who needs methadone</td>\n",
       "      <td>[who, need, methadon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399991</th>\n",
       "      <td>272567</td>\n",
       "      <td>positive</td>\n",
       "      <td>easy to make</td>\n",
       "      <td>[easi, to, make]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399992</th>\n",
       "      <td>69662</td>\n",
       "      <td>negative</td>\n",
       "      <td>really how many sugar carb packed energy drink...</td>\n",
       "      <td>[realli, how, mani, sugar, carb, pack, energi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399993</th>\n",
       "      <td>320724</td>\n",
       "      <td>positive</td>\n",
       "      <td>a darn good pitcher of tea</td>\n",
       "      <td>[a, darn, good, pitcher, of, tea]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399994</th>\n",
       "      <td>292728</td>\n",
       "      <td>positive</td>\n",
       "      <td>love it love it</td>\n",
       "      <td>[love, it, love, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>364290</td>\n",
       "      <td>positive</td>\n",
       "      <td>works well</td>\n",
       "      <td>[work, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>89301</td>\n",
       "      <td>positive</td>\n",
       "      <td>liver biscotti dog treats</td>\n",
       "      <td>[liver, biscotti, dog, treat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>444413</td>\n",
       "      <td>positive</td>\n",
       "      <td>best crunchy granola bar</td>\n",
       "      <td>[best, crunchi, granola, bar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>98012</td>\n",
       "      <td>positive</td>\n",
       "      <td>makes the juiciest steaks and chops ever</td>\n",
       "      <td>[make, the, juiciest, steak, and, chop, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>472178</td>\n",
       "      <td>negative</td>\n",
       "      <td>made me sick</td>\n",
       "      <td>[made, me, sick]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index     Score                                            Summary  \\\n",
       "0       258745  positive                            definitely a new cereal   \n",
       "1        20568  negative                                           hated it   \n",
       "2       514420  positive                                   best treats ever   \n",
       "3        29245  positive                                    perfect pumpkin   \n",
       "4       224240  positive                                     anytime of day   \n",
       "5       143690  positive                  i am sutisufied with my purchase    \n",
       "6       184981  positive                           rj s soft black licorice   \n",
       "7        60248  positive                              exquisitely tempting    \n",
       "8       131954  positive                                   cheap and tasty    \n",
       "9       310032  positive                good lord thats a lot of bay leafs    \n",
       "10      421023  positive            not the kind of smoked salmon i wanted    \n",
       "11      246737  positive                                         good stuff   \n",
       "12      467560  negative                         diamond pet foods recalled   \n",
       "13      512825  positive                                       love lavazza   \n",
       "14      312829  positive                             provolone vs provolone   \n",
       "15      498891  positive                 ex s cat likes it need i say more    \n",
       "16       17355  positive                       good tea reusable canisters    \n",
       "17       90371  positive                       great deal i love these bars   \n",
       "18        6158  negative                              tinged with vitamin c   \n",
       "19      261032  positive                                            yum eee   \n",
       "20      216517  positive                                    tug jug dog toy   \n",
       "21      381787  positive                                 trident val u pack   \n",
       "22      283486  positive                            my 10th favorite flavor   \n",
       "23      236757  positive                                         wonderful    \n",
       "24      425748  positive                                    crackers o boy    \n",
       "25      314525  positive  fantastic option for the vegetarian baby and c...   \n",
       "26       53457  positive               excellent healthy food for your dog    \n",
       "27      462286  positive                                 yummi for the cats   \n",
       "28       94868  positive                                                yum   \n",
       "29      525514  positive                              interesting packaging   \n",
       "...        ...       ...                                                ...   \n",
       "399970  496507  negative                  warning very high sodium content    \n",
       "399971  350747  positive                                   i like the taste   \n",
       "399972  467156  positive                         everytime fabulous risotto   \n",
       "399973   26817  positive                             just like the theater    \n",
       "399974   99050  positive                                           love it    \n",
       "399975  347328  positive                                 great catnip seeds   \n",
       "399976  105166  positive                          a wonderful little cookie   \n",
       "399977  397313  positive                          good on almost everything   \n",
       "399978  196485  positive                                             great    \n",
       "399979  444966  positive                               way to make converts   \n",
       "399980  377692  positive                                great hot chocolate   \n",
       "399981  482186  positive                   perfect for the hot tea drinker    \n",
       "399982  484316  positive                                      tastes great    \n",
       "399983  419313  negative  i love dark coffee but this isn t the one for me    \n",
       "399984  466307  positive                                  my favorite drink   \n",
       "399985   48371  negative                                        close call    \n",
       "399986  365437  negative               should have listened to the warnings   \n",
       "399987  458304  positive                           the best steak seasoning   \n",
       "399988  442488  positive                                          dee lish    \n",
       "399989   51984  positive                             better treat than meal   \n",
       "399990  267349  positive                               who needs methadone    \n",
       "399991  272567  positive                                      easy to make    \n",
       "399992   69662  negative  really how many sugar carb packed energy drink...   \n",
       "399993  320724  positive                         a darn good pitcher of tea   \n",
       "399994  292728  positive                                   love it love it    \n",
       "399995  364290  positive                                         works well   \n",
       "399996   89301  positive                          liver biscotti dog treats   \n",
       "399997  444413  positive                          best crunchy granola bar    \n",
       "399998   98012  positive           makes the juiciest steaks and chops ever   \n",
       "399999  472178  negative                                       made me sick   \n",
       "\n",
       "                                                 Summary1  \n",
       "0                               [definit, a, new, cereal]  \n",
       "1                                              [hate, it]  \n",
       "2                                     [best, treat, ever]  \n",
       "3                                      [perfect, pumpkin]  \n",
       "4                                       [anytim, of, day]  \n",
       "5                    [i, am, sutisufi, with, my, purchas]  \n",
       "6                           [rj, s, soft, black, licoric]  \n",
       "7                                       [exquisit, tempt]  \n",
       "8                                     [cheap, and, tasti]  \n",
       "9               [good, lord, that, a, lot, of, bay, leaf]  \n",
       "10           [not, the, kind, of, smoke, salmon, i, want]  \n",
       "11                                          [good, stuff]  \n",
       "12                            [diamond, pet, food, recal]  \n",
       "13                                        [love, lavazza]  \n",
       "14                               [provolon, vs, provolon]  \n",
       "15             [ex, s, cat, like, it, need, i, say, more]  \n",
       "16                           [good, tea, reusabl, canist]  \n",
       "17                     [great, deal, i, love, these, bar]  \n",
       "18                               [ting, with, vitamin, c]  \n",
       "19                                             [yum, eee]  \n",
       "20                                   [tug, jug, dog, toy]  \n",
       "21                                [trident, val, u, pack]  \n",
       "22                            [my, 10th, favorit, flavor]  \n",
       "23                                               [wonder]  \n",
       "24                                      [cracker, o, boy]  \n",
       "25      [fantast, option, for, the, vegetarian, babi, ...  \n",
       "26                 [excel, healthi, food, for, your, dog]  \n",
       "27                                 [yummi, for, the, cat]  \n",
       "28                                                  [yum]  \n",
       "29                                     [interest, packag]  \n",
       "...                                                   ...  \n",
       "399970                [warn, veri, high, sodium, content]  \n",
       "399971                               [i, like, the, tast]  \n",
       "399972                         [everytim, fabul, risotto]  \n",
       "399973                         [just, like, the, theater]  \n",
       "399974                                         [love, it]  \n",
       "399975                              [great, catnip, seed]  \n",
       "399976                          [a, wonder, littl, cooki]  \n",
       "399977                        [good, on, almost, everyth]  \n",
       "399978                                            [great]  \n",
       "399979                           [way, to, make, convert]  \n",
       "399980                               [great, hot, chocol]  \n",
       "399981             [perfect, for, the, hot, tea, drinker]  \n",
       "399982                                      [tast, great]  \n",
       "399983  [i, love, dark, coffe, but, thi, isn, t, the, ...  \n",
       "399984                               [my, favorit, drink]  \n",
       "399985                                      [close, call]  \n",
       "399986              [should, have, listen, to, the, warn]  \n",
       "399987                         [the, best, steak, season]  \n",
       "399988                                        [dee, lish]  \n",
       "399989                        [better, treat, than, meal]  \n",
       "399990                              [who, need, methadon]  \n",
       "399991                                   [easi, to, make]  \n",
       "399992  [realli, how, mani, sugar, carb, pack, energi,...  \n",
       "399993                  [a, darn, good, pitcher, of, tea]  \n",
       "399994                               [love, it, love, it]  \n",
       "399995                                       [work, well]  \n",
       "399996                      [liver, biscotti, dog, treat]  \n",
       "399997                      [best, crunchi, granola, bar]  \n",
       "399998      [make, the, juiciest, steak, and, chop, ever]  \n",
       "399999                                   [made, me, sick]  \n",
       "\n",
       "[400000 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledf=df.sample(n=400000)\n",
    "sampledf.reset_index(inplace=True)\n",
    "sampledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300001,)\n",
      "(100000,)\n",
      "(300001,)\n",
      "(100000,)\n"
     ]
    }
   ],
   "source": [
    "X_train=sampledf.loc[:300000,'Summary'].values\n",
    "y_train=sampledf.loc[:300000,'Score'].values\n",
    "X_test=sampledf.loc[300000:,'Summary'].values\n",
    "y_test=sampledf.loc[300000:,'Score'].values\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 72 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 288 out of 288 | elapsed: 73.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__norm': 'l2', 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000002AB302D4510>}\n",
      "4442.774648189545\n"
     ]
    }
   ],
   "source": [
    "#Using GridSearchCV obj for LogisticReg. using 5-fold cross-val\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "\n",
    "param_grid=[{'vect__ngram_range':[(1,1)],\n",
    "             'vect__stop_words':[stop,None],\n",
    "             'vect__tokenizer':[tokenizer,tokenize_porter],\n",
    "             'vect__norm':['l1','l2'],\n",
    "             'clf__penalty':['l1','l2'],\n",
    "             'clf__C':[1.0,10.0,20.0]},\n",
    "            {'vect__ngram_range': [(1,1)],\n",
    "             'vect__stop_words': [stop, None],\n",
    "             'vect__tokenizer': [tokenizer,tokenize_porter],\n",
    "             'vect__use_idf':[False],\n",
    "             'vect__norm':[None],\n",
    "             'clf__penalty':['l1', 'l2'],\n",
    "             'clf__C': [1.0, 10.0,20.0]}]\n",
    "                \n",
    "lr_tfidf=Pipeline([('vect',tfidf),('clf',LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf=GridSearchCV(lr_tfidf,param_grid,scoring='accuracy',cv=4,verbose=1,n_jobs=1)\n",
    "\n",
    "\n",
    "import time\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train,y_train)\n",
    "end=time.time()\n",
    "\n",
    "print('Best Parameter Set: %s'%gs_lr_tfidf.best_params_)\n",
    "print(end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Train Accu. Score : 0.943 \n",
      "LogReg Train F1 Score : 0.943 \n",
      "LogReg Train Precision Score : 0.943 \n",
      "LogReg Train Recall Score : 0.943 \n",
      "LogReg Train Confusion Matrix\n",
      " [[ 35005  11721]\n",
      " [  5437 247838]]\n",
      "LogReg Test Accu. Score : 0.929 \n",
      "LogReg Test F1 Score : 0.929 \n",
      "LogReg Test Precision Score : 0.929 \n",
      "LogReg Test Recall Score : 0.929 \n",
      "LogReg Test Confusion Matrix\n",
      " [[10871  4823]\n",
      " [ 2273 82033]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score\n",
    "\n",
    "prediction_lr_big_train=gs_lr_tfidf.predict(X_train)\n",
    "\n",
    "print('LogReg Train Accu. Score : %.3f ' %(accuracy_score(y_train,prediction_lr_big_train,normalize=True)))\n",
    "print('LogReg Train F1 Score : %.3f ' %(f1_score(y_train,prediction_lr_big_train,average='micro')))\n",
    "print('LogReg Train Precision Score : %.3f ' %precision_score(y_train,prediction_lr_big_train,average='micro'))\n",
    "print('LogReg Train Recall Score : %.3f ' %recall_score(y_train,prediction_lr_big_train,average='micro'))\n",
    "print('LogReg Train Confusion Matrix\\n' ,confusion_matrix(y_train,prediction_lr_big_train))\n",
    "\n",
    "\n",
    "prediction_lr_big_test=gs_lr_tfidf.predict(X_test)\n",
    "\n",
    "print('LogReg Test Accu. Score : %.3f ' %(accuracy_score(y_test,prediction_lr_big_test,normalize=True)))\n",
    "print('LogReg Test F1 Score : %.3f ' %(f1_score(y_test,prediction_lr_big_test,average='micro')))\n",
    "print('LogReg Test Precision Score : %.3f ' %precision_score(y_test,prediction_lr_big_test,average='micro'))\n",
    "print('LogReg Test Recall Score : %.3f ' %recall_score(y_test,prediction_lr_big_test,average='micro'))\n",
    "print('LogReg Test Confusion Matrix\\n' ,confusion_matrix(y_test,prediction_lr_big_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
